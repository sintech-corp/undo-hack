{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "excel_file_path = 'data.xlsx'\n",
    "new_excel_file_path = 'Aplicanti_Compensatii_Locatari.xlsx'\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "df_original = pd.read_excel(new_excel_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Proccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Id  Grad         Raion  \\\n",
      "0       0000592c-2e90-4a36-9e39-8aabd287aa93     4  mun.Chişinău   \n",
      "2       000088d5-b06f-44df-a20e-a7d13bc84028     4  mun.Chişinău   \n",
      "4       00010c3f-1305-4031-869a-580b39fc2a75     3  mun.Chişinău   \n",
      "6       00013c26-465e-4ca1-9b84-c6859e2b634d     4  mun.Chişinău   \n",
      "9       00020e46-9b16-4fdd-a437-d59ffccf2844     0  mun.Chişinău   \n",
      "...                                      ...   ...           ...   \n",
      "251571  a51f9aa6-ee7e-41ff-8d14-b7137af29f7a     4  mun.Chişinău   \n",
      "251573  a51fb7f4-a5ba-47b6-b7ca-f3afeff20223     2  mun.Chişinău   \n",
      "251577  a5201656-da31-4d70-bb72-3f2381f1524b     4  mun.Chişinău   \n",
      "251579  a5202710-bf59-40bf-964d-48947a7b1088     4  mun.Chişinău   \n",
      "251581  a5202eaa-84b7-42bf-8f32-f81188248a7d     4  mun.Chişinău   \n",
      "\n",
      "          Localitate             Strada Tip incalzire principal  DateOfBirth  \\\n",
      "0         s.Bubuieci          M.Frunza            Gaze naturale       1949.0   \n",
      "2           s.Băcioi     Ioan Cuza Voda           Gaze naturale       1978.0   \n",
      "4       mun.Chişinău  str. Nicolae Dimo  Incalzire centralizata       1980.0   \n",
      "6       mun.Chişinău        Ion Pelivan  Incalzire centralizata       1963.0   \n",
      "9       mun.Chişinău     str.Igor Vieru           Gaze naturale       1945.0   \n",
      "...              ...                ...                     ...          ...   \n",
      "251571  mun.Chişinău        P. Zadnipru  Incalzire centralizata       1941.0   \n",
      "251573  mun.Chişinău        N. Zelinski  Incalzire centralizata       1986.0   \n",
      "251577  mun.Chişinău     str. Cuza Voda  Incalzire centralizata       1949.0   \n",
      "251579    s.Stăuceni      str.Nucarilor           Gaze naturale       1956.0   \n",
      "251581  mun.Chişinău      str. Zelinski  Incalzire centralizata       1945.0   \n",
      "\n",
      "        Sex  AverageIncome  CompensatedEnergySource AlternativeEnergySources  \\\n",
      "0       2.0         2450.0                      1.0                      [2]   \n",
      "2       2.0         5000.0                      1.0                      [2]   \n",
      "4       1.0            0.0                      2.0                       []   \n",
      "6       2.0         2245.0                      2.0                       []   \n",
      "9       1.0        21000.0                      1.0                      [2]   \n",
      "...     ...            ...                      ...                      ...   \n",
      "251571  2.0         2400.0                      2.0                       []   \n",
      "251573  2.0        12000.0                      2.0                       []   \n",
      "251577  2.0         4143.0                      2.0                       []   \n",
      "251579  1.0         3740.0                      1.0                    [2,4]   \n",
      "251581  2.0         4884.0                      2.0                       []   \n",
      "\n",
      "        EnergyType                Name  NrMembriDeFamilie  \n",
      "0              0.0  PREMIER ENERGY SRL                1.0  \n",
      "2              0.0  PREMIER ENERGY SRL                2.0  \n",
      "4              0.0  PREMIER ENERGY SRL                0.0  \n",
      "6              0.0  PREMIER ENERGY SRL                0.0  \n",
      "9              0.0  PREMIER ENERGY SRL                1.0  \n",
      "...            ...                 ...                ...  \n",
      "251571         0.0  PREMIER ENERGY SRL                1.0  \n",
      "251573         0.0  PREMIER ENERGY SRL                0.0  \n",
      "251577         0.0  PREMIER ENERGY SRL                0.0  \n",
      "251579         0.0  PREMIER ENERGY SRL                1.0  \n",
      "251581         0.0  PREMIER ENERGY SRL                1.0  \n",
      "\n",
      "[123630 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime# Get the current year\n",
    "\n",
    "df_copy = df_original.copy(deep=True)\n",
    "\n",
    "# Remove consum\n",
    "df_copy.drop([\"Consum volum 11.2022\",\"Consum volum 12.2022\",\"Consum volum 01.2023\",\"Consum volum 02.2023\",\"Consum volum 03.2023\"],axis=1, inplace=True)\n",
    "\n",
    "# Remove all NaN\n",
    "df_copy = df_copy.dropna()\n",
    "\n",
    "print(df_copy)\n",
    "\n",
    "# Sex 0 -> 1\n",
    "df_copy['Sex'] = df_copy['Sex'].replace(0, 1)\n",
    "\n",
    "# Age ranging \n",
    "current_year = datetime.now().year\n",
    "df_copy['Age'] = current_year - df_copy['DateOfBirth']\n",
    "age_ranges = [0, 20, 40, 60, 80, 100]\n",
    "labels = ['0-20', '20-40', '40-60', '60-80', '80-100']\n",
    "df_copy['AgeRange'] = pd.cut(df_copy['Age'], bins=age_ranges, labels=labels, include_lowest=True,ordered=False)\n",
    "df_copy['AgeRange'] = df_copy['AgeRange'].astype(str)\n",
    "\n",
    "# Salary ranging\n",
    "salary_ranges = [0, 10000,20000, 40000, 60000, 80000, 100000]\n",
    "labels = ['0-10k','10-20k', '20k-40k', '40k-60k', '60k-80k', '80k-100k']\n",
    "df_copy['SalaryRange'] = pd.cut(df_copy['AverageIncome'], bins=salary_ranges, labels=labels, include_lowest=True,ordered=False)\n",
    "df_copy['SalaryRange'] = df_copy['SalaryRange'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          8\n",
      "2         12\n",
      "4          0\n",
      "6          0\n",
      "9          0\n",
      "          ..\n",
      "251571     0\n",
      "251573     0\n",
      "251577     0\n",
      "251579    31\n",
      "251581     0\n",
      "Name: Localitate_encoded, Length: 123630, dtype: int64\n",
      "0         2\n",
      "2         2\n",
      "4         3\n",
      "6         3\n",
      "9         2\n",
      "         ..\n",
      "251571    3\n",
      "251573    3\n",
      "251577    3\n",
      "251579    2\n",
      "251581    3\n",
      "Name: Tip_incalzire_principal_encoded, Length: 123630, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "salary_range_label_encoder = LabelEncoder()\n",
    "age_range_label_encoder = LabelEncoder()\n",
    "localitate_range_label_encoder = LabelEncoder()\n",
    "tip_incalzire_principal_label_encoder = LabelEncoder()\n",
    "company_name_label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'Category' column\n",
    "df_copy['SalaryRange_encoded'] = salary_range_label_encoder.fit_transform(df_copy['SalaryRange'])\n",
    "df_copy['AgeRange_encoded'] = age_range_label_encoder.fit_transform(df_copy['AgeRange'])\n",
    "df_copy['Localitate_encoded'] = localitate_range_label_encoder.fit_transform(df_copy['Localitate'])\n",
    "df_copy['Tip_incalzire_principal_encoded'] = tip_incalzire_principal_label_encoder.fit_transform(df_copy['Tip incalzire principal'])\n",
    "df_copy['Company_name_encoded'] = company_name_label_encoder.fit_transform(df_copy['Name'])\n",
    "\n",
    "\n",
    "\n",
    "print(df_copy[\"Localitate_encoded\"])\n",
    "print(df_copy[\"Tip_incalzire_principal_encoded\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection with KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = df_copy[['Grad','Sex','SalaryRange_encoded', 'AgeRange_encoded','Localitate_encoded','Tip_incalzire_principal_encoded','Company_name_encoded']]\n",
    "\n",
    "# Standardize the feature columns\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "print(len(features_scaled))\n",
    " \n",
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df_copy['Cluster'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# Visualize the clustering (assuming 2D features for simplicity)\n",
    "# plt.scatter(features_scaled[:, 0], features_scaled[:, 2], c=df_copy['Cluster'], cmap='viridis')\n",
    "# plt.title('K-Means Clustering')\n",
    "# plt.xlabel('Feature1')\n",
    "# plt.ylabel('Feature2')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "for cluster_id in range(5):\n",
    "    cluster_data = features_scaled[df_copy['Cluster'] == cluster_id]\n",
    "    \n",
    "    isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    df_copy.loc[df_copy['Cluster'] == cluster_id, 'IsOutlier'] = isolation_forest.fit_predict(cluster_data)\n",
    "\n",
    "# Visualize anomalies\n",
    "anomalies = df_copy[df_copy['IsOutlier'] == -1]\n",
    "# plt.scatter(features_scaled[:, 0], features_scaled[:, 2], c=df_copy['Cluster'], cmap='viridis', label='Normal')\n",
    "# plt.scatter(anomalies['Grad'], anomalies['SalaryRange_encoded'], color='red', label='Anomaly')\n",
    "# plt.title('Anomaly Detection')\n",
    "# plt.xlabel('Feature1')\n",
    "# plt.ylabel('Feature2')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "normal_data = df_copy[df_copy['IsOutlier'] == 1]\n",
    "anomaly_data = df_copy[df_copy['IsOutlier'] == -1]\n",
    "\n",
    "# Plot a scatter plot for Vulnerability vs Feature1 for anomalies\n",
    "# plt.scatter(anomaly_data['Localitate_encoded'], anomaly_data['SalaryRange_encoded'], c='red', label='Anomalies')\n",
    "# plt.title('Vulnerability vs Feature1 for Anomalies')\n",
    "# plt.xlabel('Location')\n",
    "# plt.ylabel('Salary')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Plot a scatter plot for Feature1 vs Feature2 with color-coded clusters\n",
    "plt.scatter(df_copy['SalaryRange_encoded'], df_copy['AgeRange_encoded'], c=df_copy['Cluster'], cmap='viridis', edgecolor='k')\n",
    "plt.title('Feature1 vs Feature2 with K-Means Clusters')\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Age')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(len(normal_data))\n",
    "print(len(anomaly_data))\n",
    "\n",
    "# Filter data for anomalies\n",
    "# Inspect decision function scores\n",
    "anomaly_scores = isolation_forest.fit_predict(features_scaled)\n",
    "print(anomaly_scores)\n",
    "# Identify top contributing features for anomalies\n",
    "top_contributing_features = features.columns[anomaly_scores.argsort()[:5]]  # Adjust the number of top features as needed\n",
    "\n",
    "print(\"Top Contributing Features for Anomalies:\")\n",
    "print(top_contributing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consum volum 11.2022</th>\n",
       "      <th>Consum volum 12.2022</th>\n",
       "      <th>Consum volum 01.2023</th>\n",
       "      <th>Consum volum 02.2023</th>\n",
       "      <th>Consum volum 03.2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>358.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251571</th>\n",
       "      <td>33.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251573</th>\n",
       "      <td>44.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251577</th>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251579</th>\n",
       "      <td>69.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251581</th>\n",
       "      <td>37.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123630 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Consum volum 11.2022  Consum volum 12.2022  Consum volum 01.2023  \\\n",
       "0                      358.0                 416.0                 387.0   \n",
       "2                      138.0                 149.0                 151.0   \n",
       "4                      103.0                 100.0                 103.0   \n",
       "6                       28.0                  36.0                  42.0   \n",
       "9                       84.0                  85.0                  89.0   \n",
       "...                      ...                   ...                   ...   \n",
       "251571                  33.0                  36.0                  37.0   \n",
       "251573                  44.0                  48.0                  52.0   \n",
       "251577                  31.0                  27.0                  36.0   \n",
       "251579                  69.0                  74.0                  79.0   \n",
       "251581                  37.0                  42.0                  38.0   \n",
       "\n",
       "        Consum volum 02.2023  Consum volum 03.2023  \n",
       "0                      327.0                 348.0  \n",
       "2                      135.0                 143.0  \n",
       "4                      100.0                  93.0  \n",
       "6                       36.0                  25.0  \n",
       "9                       85.0                  79.0  \n",
       "...                      ...                   ...  \n",
       "251571                  33.0                  37.0  \n",
       "251573                  61.0                  67.0  \n",
       "251577                  32.0                  25.0  \n",
       "251579                  76.0                  68.0  \n",
       "251581                  35.0                  36.0  \n",
       "\n",
       "[123630 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy_consum[[\"Consum volum 11.2022\",\"Consum volum 12.2022\",\"Consum volum 01.2023\",\"Consum volum 02.2023\",\"Consum volum 03.2023\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection using Grad as Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1.0\n",
      "2         1.0\n",
      "4         1.0\n",
      "6         1.0\n",
      "9         1.0\n",
      "         ... \n",
      "251571    1.0\n",
      "251573    1.0\n",
      "251577    1.0\n",
      "251579   -1.0\n",
      "251581    1.0\n",
      "Name: IsOutlier, Length: 123630, dtype: float64\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     df_copy\u001b[39m.\u001b[39mloc[df_copy[\u001b[39m'\u001b[39m\u001b[39mGrad\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m cluster_id, \u001b[39m'\u001b[39m\u001b[39mIsOutlier\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m isolation_forest\u001b[39m.\u001b[39mfit_predict(cluster_data)\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(df_copy[\u001b[39m'\u001b[39m\u001b[39mIsOutlier\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 24\u001b[0m df_copy \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mmerge(df_copy, df_copy_consum[[\u001b[39m\"\u001b[39;49m\u001b[39mConsum volum 11.2022\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mConsum volum 12.2022\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mConsum volum 01.2023\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mConsum volum 02.2023\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mConsum volum 03.2023\u001b[39;49m\u001b[39m\"\u001b[39;49m]], how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mright\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m df_copy\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mNew_data.csv\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/ProtonEnergySolutions/Software/Hackathon/UNDP/.venv/lib/python3.9/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    111\u001b[0m         left,\n\u001b[1;32m    112\u001b[0m         right,\n\u001b[1;32m    113\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m    114\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m    115\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m    116\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m    117\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m    118\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m    119\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    120\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m    121\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m    122\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/Desktop/ProtonEnergySolutions/Software/Hackathon/UNDP/.venv/lib/python3.9/site-packages/pandas/core/reshape/merge.py:685\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[39m# stacklevel chosen to be correct when this is reached via pd.merge\u001b[39;00m\n\u001b[1;32m    682\u001b[0m     \u001b[39m# (and not DataFrame.join)\u001b[39;00m\n\u001b[1;32m    683\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39mfind_stack_level())\n\u001b[0;32m--> 685\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_on, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_on \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_left_right_on(left_on, right_on)\n\u001b[1;32m    687\u001b[0m cross_col \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhow \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcross\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/ProtonEnergySolutions/Software/Hackathon/UNDP/.venv/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1434\u001b[0m, in \u001b[0;36m_MergeOperation._validate_left_right_on\u001b[0;34m(self, left_on, right_on)\u001b[0m\n\u001b[1;32m   1432\u001b[0m common_cols \u001b[39m=\u001b[39m left_cols\u001b[39m.\u001b[39mintersection(right_cols)\n\u001b[1;32m   1433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(common_cols) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1434\u001b[0m     \u001b[39mraise\u001b[39;00m MergeError(\n\u001b[1;32m   1435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo common columns to perform merge on. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1436\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMerge options: left_on=\u001b[39m\u001b[39m{\u001b[39;00mleft_on\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1437\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mright_on=\u001b[39m\u001b[39m{\u001b[39;00mright_on\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1438\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft_index=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_index\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1439\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mright_index=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_index\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1440\u001b[0m     )\n\u001b[1;32m   1441\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1442\u001b[0m     \u001b[39mnot\u001b[39;00m left_cols\u001b[39m.\u001b[39mjoin(common_cols, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mis_unique\n\u001b[1;32m   1443\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m right_cols\u001b[39m.\u001b[39mjoin(common_cols, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mis_unique\n\u001b[1;32m   1444\u001b[0m ):\n\u001b[1;32m   1445\u001b[0m     \u001b[39mraise\u001b[39;00m MergeError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData columns not unique: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(common_cols)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mMergeError\u001b[0m: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = df_copy[[\"Grad\",'Sex','SalaryRange_encoded', 'AgeRange_encoded','Localitate_encoded','Tip_incalzire_principal_encoded','Company_name_encoded','NrMembriDeFamilie']]\n",
    "\n",
    "# Standardize the feature columns\n",
    "# scaler = StandardScaler()\n",
    "# features_scaled = scaler.fit_transform(features)\n",
    "# print(len(features_scaled))\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "for cluster_id in [0,1,2,3,4]:\n",
    "    cluster_data = features_normalized[df_copy['Grad'] == cluster_id]\n",
    "    \n",
    "    isolation_forest = IsolationForest(contamination=0.10, random_state=42)\n",
    "    df_copy.loc[df_copy['Grad'] == cluster_id, 'IsOutlier'] = isolation_forest.fit_predict(cluster_data)\n",
    "\n",
    "print(df_copy['IsOutlier'])\n",
    "df_copy = pd.merge(df_copy, df_copy_consum[[\"Consum volum 11.2022\",\"Consum volum 12.2022\",\"Consum volum 01.2023\",\"Consum volum 02.2023\",\"Consum volum 03.2023\"]], on='Id', how='right')\n",
    "df_copy.to_csv(\"New_data.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Id  Grad         Raion  \\\n",
      "32      000474fe-0d03-46ac-b9fe-218b539d2a85     4  mun.Chişinău   \n",
      "43      0005c982-6dcb-4950-8f9b-c908db79fa85     4  mun.Chişinău   \n",
      "58      00071ceb-57ec-4f51-97ec-25b6700b42d4     3  mun.Chişinău   \n",
      "64      0008952a-05ed-42a9-8a32-cd83412d5da2     2  mun.Chişinău   \n",
      "80      000a4f2d-8a87-45a4-810b-be1680a5dfa8     3  mun.Chişinău   \n",
      "...                                      ...   ...           ...   \n",
      "251502  a51596f7-f7e4-4bb5-b227-643a7584bf37     4  mun.Chişinău   \n",
      "251513  a5181f99-358b-4fd1-bbed-46572b9f5d66     4  mun.Chişinău   \n",
      "251564  a51ec2e1-7ee9-40cf-a869-c9cab2805f0b     3  mun.Chişinău   \n",
      "251568  a51f600f-74be-4ced-b4a6-715a5e74606c     4  mun.Chişinău   \n",
      "251579  a5202710-bf59-40bf-964d-48947a7b1088     4  mun.Chişinău   \n",
      "\n",
      "          Localitate                Strada Tip incalzire principal  \\\n",
      "32         s.Truşeni             Basarabia       Combustibil solid   \n",
      "43      mun.Chişinău          Malina Mica   Incalzire centralizata   \n",
      "58        s.Ciorescu           Str. Pavlov           Gaze naturale   \n",
      "64        s.Bubuieci   str-la Tineretului            Gaze naturale   \n",
      "80      s.Ghidighici                Serban           Gaze naturale   \n",
      "...              ...                   ...                     ...   \n",
      "251502    s.Stăuceni                Unirii           Gaze naturale   \n",
      "251513    or.Cricova       Bogdan Voievod            Gaze naturale   \n",
      "251564    s.Ciorescu   Str. Trandafirilor            Gaze naturale   \n",
      "251568  mun.Chişinău  Valea trandafirilor   Incalzire centralizata   \n",
      "251579    s.Stăuceni         str.Nucarilor           Gaze naturale   \n",
      "\n",
      "        DateOfBirth  Sex  AverageIncome  CompensatedEnergySource  ...  \\\n",
      "32           1970.0  2.0         5100.0                      3.0  ...   \n",
      "43           1941.0  1.0        12099.0                      2.0  ...   \n",
      "58           1991.0  1.0        14000.0                      1.0  ...   \n",
      "64           1986.0  1.0         9400.0                      1.0  ...   \n",
      "80           1963.0  2.0        13200.0                      1.0  ...   \n",
      "...             ...  ...            ...                      ...  ...   \n",
      "251502       1994.0  2.0            0.0                      1.0  ...   \n",
      "251513       1973.0  1.0        21000.0                      1.0  ...   \n",
      "251564       1960.0  1.0         5400.0                      1.0  ...   \n",
      "251568       1987.0  1.0        48000.0                      2.0  ...   \n",
      "251579       1956.0  1.0         3740.0                      1.0  ...   \n",
      "\n",
      "       NrMembriDeFamilie   Age AgeRange  SalaryRange  SalaryRange_encoded  \\\n",
      "32                   0.0  53.0    40-60        0-10k                    0   \n",
      "43                   1.0  82.0   80-100       10-20k                    1   \n",
      "58                   3.0  32.0    20-40       10-20k                    1   \n",
      "64                   3.0  37.0    20-40        0-10k                    0   \n",
      "80                   0.0  60.0    40-60       10-20k                    1   \n",
      "...                  ...   ...      ...          ...                  ...   \n",
      "251502               0.0  29.0    20-40        0-10k                    0   \n",
      "251513               0.0  50.0    40-60      20k-40k                    2   \n",
      "251564               1.0  63.0    60-80        0-10k                    0   \n",
      "251568               5.0  36.0    20-40      40k-60k                    3   \n",
      "251579               1.0  67.0    60-80        0-10k                    0   \n",
      "\n",
      "       AgeRange_encoded Localitate_encoded  Tip_incalzire_principal_encoded  \\\n",
      "32                    2                 33                                0   \n",
      "43                    4                  0                                3   \n",
      "58                    1                 15                                2   \n",
      "64                    1                  8                                2   \n",
      "80                    2                 23                                2   \n",
      "...                 ...                ...                              ...   \n",
      "251502                1                 31                                2   \n",
      "251513                2                  2                                2   \n",
      "251564                3                 15                                2   \n",
      "251568                1                  0                                3   \n",
      "251579                3                 31                                2   \n",
      "\n",
      "        Company_name_encoded  IsOutlier  \n",
      "32                         5       -1.0  \n",
      "43                         5       -1.0  \n",
      "58                         5       -1.0  \n",
      "64                         5       -1.0  \n",
      "80                         5       -1.0  \n",
      "...                      ...        ...  \n",
      "251502                     5       -1.0  \n",
      "251513                     5       -1.0  \n",
      "251564                     5       -1.0  \n",
      "251568                     5       -1.0  \n",
      "251579                     5       -1.0  \n",
      "\n",
      "[12337 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_copy[df_copy['IsOutlier'] == -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afanasichihaioglo/Desktop/ProtonEnergySolutions/Software/Hackathon/UNDP/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9639515759389576\n",
      "Precision: 0.9705511395301089\n",
      "Recall: 0.99003650290228\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n",
    "\n",
    "features = df_copy[['Grad','Sex','SalaryRange_encoded', 'AgeRange_encoded','Localitate_encoded','Tip_incalzire_principal_encoded','Company_name_encoded','NrMembriDeFamilie']]\n",
    "target = df_copy[[\"IsOutlier\"]]\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3,random_state=109) # 70% training and 30% test\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "with open('svm_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(clf, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/fb34lf_n3wd44716f74347tc0000gn/T/ipykernel_8238/2641502145.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  row_to_predict = row_to_predict.append({'Grad':data_dict['Grad'],'Sex':data_dict['Sex'],'SalaryRange_encoded':data_dict[\"SalaryRange_encoded\"], 'AgeRange_encoded':data_dict[\"AgeRange_encoded\"],'Localitate_encoded':data_dict[\"Localitate_encoded\"],'Tip_incalzire_principal_encoded':data_dict[\"Tip_incalzire_principal_encoded\"],'Company_name_encoded':data_dict[\"Company_name_encoded\"],'NrMembriDeFamilie':data_dict[\"NrMembriDeFamilie\"]}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def return_prediction(model_file, data_dict):\n",
    "    # x = ['Grad','Sex','SalaryRange_encoded', 'AgeRange_encoded','Localitate_encoded','Tip_incalzire_principal_encoded','Company_name_encoded','NrMembriDeFamilie']\n",
    "    row_to_predict = pd.DataFrame(columns=['Grad','Sex','SalaryRange_encoded', 'AgeRange_encoded','Localitate_encoded','Tip_incalzire_principal_encoded','Company_name_encoded','NrMembriDeFamilie'])\n",
    "    row_to_predict = row_to_predict.append({'Grad':data_dict['Grad'],'Sex':data_dict['Sex'],'SalaryRange_encoded':data_dict[\"SalaryRange_encoded\"], 'AgeRange_encoded':data_dict[\"AgeRange_encoded\"],'Localitate_encoded':data_dict[\"Localitate_encoded\"],'Tip_incalzire_principal_encoded':data_dict[\"Tip_incalzire_principal_encoded\"],'Company_name_encoded':data_dict[\"Company_name_encoded\"],'NrMembriDeFamilie':data_dict[\"NrMembriDeFamilie\"]}, ignore_index=True)\n",
    "    \n",
    "    with open('svm_model.pkl', 'rb') as model_file:\n",
    "        loaded_model = pickle.load(model_file)\n",
    "\n",
    "    # Make predictions with the loaded model\n",
    "    new_predictions = loaded_model.predict(row_to_predict)\n",
    "    print(new_predictions)\n",
    "\n",
    "data = {'Grad':4,'Sex':1,'SalaryRange_encoded':1, 'AgeRange_encoded':4,'Localitate_encoded':1,'Tip_incalzire_principal_encoded':1,'Company_name_encoded':1,'NrMembriDeFamilie':10}\n",
    "model_file = \"/Users/afanasichihaioglo/Desktop/ProtonEnergySolutions/Software/Hackathon/UNDP/svm_model.pkl\"\n",
    "\n",
    "return_prediction(model_file,data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features and their absolute coefficients:\n",
      "SalaryRange_encoded: 1.1686931347221616\n",
      "Tip_incalzire_principal_encoded: 0.9725490992489085\n",
      "Grad: 0.4902585748095589\n",
      "Sex: 0.4747998038625383\n",
      "NrMembriDeFamilie: 0.3371777154316078\n",
      "Company_name_encoded: 0.20789929232523718\n",
      "Localitate_encoded: 0.15024242022673207\n",
      "AgeRange_encoded: 0.09820137363976755\n",
      "Prediction: [1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/fb34lf_n3wd44716f74347tc0000gn/T/ipykernel_8238/812595987.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  row_to_predict = row_to_predict.append({'Grad':0,'Sex':1,'SalaryRange_encoded':0, 'AgeRange_encoded':4,'Localitate_encoded':1,'Tip_incalzire_principal_encoded':1,'Company_name_encoded':1,'NrMembriDeFamilie':5}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Select one row from X_test for prediction (change the index as needed)\n",
    "# row_to_predict = X_test.iloc[[34512]]\n",
    "# print(type((row_to_predict)))\n",
    "# row_to_predict = pd.DataFrame([1,4, 0,1,1,1,2])\n",
    "# row_to_predict = pd.DataFrame({'Sex':1,'SalaryRange_encoded':4, 'AgeRange_encoded':0,'Localitate_encoded':1,'Tip_incalzire_principal_encoded':1,'Company_name_encoded':1,'NrMembriDeFamilie':2})\n",
    "import numpy as np\n",
    "row_to_predict = pd.DataFrame(columns=['Grad','Sex','SalaryRange_encoded', 'AgeRange_encoded','Localitate_encoded','Tip_incalzire_principal_encoded','Company_name_encoded','NrMembriDeFamilie'])\n",
    "row_to_predict = row_to_predict.append({'Grad':0,'Sex':1,'SalaryRange_encoded':0, 'AgeRange_encoded':4,'Localitate_encoded':1,'Tip_incalzire_principal_encoded':1,'Company_name_encoded':1,'NrMembriDeFamilie':5}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Predict the response for the selected row\n",
    "prediction = clf.predict(row_to_predict)\n",
    "\n",
    "# Get the absolute values of the coefficients\n",
    "abs_coefficients = np.abs(clf.coef_)\n",
    "\n",
    "# Create a dictionary mapping feature names to their absolute coefficients\n",
    "feature_coefficients = dict(zip(features.columns, abs_coefficients[0]))\n",
    "\n",
    "# Sort the features by their absolute coefficients in descending order\n",
    "top_features = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top features\n",
    "print(\"Top features and their absolute coefficients:\")\n",
    "for feature, coefficient in top_features:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "\n",
    "# # Predict the response for the selected row\n",
    "# prediction = clf.predict(row_to_predict)\n",
    "\n",
    "# # Get indices of support vectors\n",
    "# support_vector_indices = clf.support_\n",
    "\n",
    "# # Get the corresponding support vectors\n",
    "# support_vectors = X_train.iloc[support_vector_indices]\n",
    "\n",
    "# # Print the prediction\n",
    "# # print(\"Prediction:\", prediction)\n",
    "\n",
    "# print(support_vectors)\n",
    "\n",
    "# # Print the features and their values for the support vectors\n",
    "# # print(\"Features and their values for support vectors:\")\n",
    "# # for index, row in support_vectors.iterrows():\n",
    "# #     print(row)\n",
    "\n",
    "# # You can also check the dual coefficients (dual_coef_) associated with the support vectors\n",
    "# print(\"Dual coefficients of support vectors:\", clf.dual_coef_[0])\n",
    "\n",
    "# # Get the coefficients of the SVM model (weights)\n",
    "# coefficients = clf.coef_[0]\n",
    "\n",
    "# # Print the prediction and the features along with their coefficients\n",
    "# print(\"Prediction:\", prediction)\n",
    "# print(\"Features and their contributions:\")\n",
    "# for feature, coefficient in zip(features.columns, coefficients):\n",
    "#     print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "# # If you want to print the intercept term (bias)\n",
    "# print(\"Intercept (Bias):\", clf.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Assuming df_copy is your DataFrame\n",
    "features = df_copy[[\"Sex\", \"SalaryRange_encoded\", \"AgeRange_encoded\", \"Localitate_encoded\", \"Tip_incalzire_principal_encoded\", \"Company_name_encoded\", \"NrMembriDeFamilie\"]]\n",
    "target = df_copy[\"IsOutlier\"]\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=109)  # 70% training and 30% test\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Compute precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Compute recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one row from X_test for prediction (change the index as needed)\n",
    "row_to_predict = X_test_scaled[[2000]]\n",
    "row_to_predict = pd.DataFrame(columns=['Sex','SalaryRange_encoded', 'AgeRange_encoded','Localitate_encoded','Tip_incalzire_principal_encoded','Company_name_encoded','NrMembriDeFamilie'])\n",
    "row_to_predict = row_to_predict.append({'Sex':1,'SalaryRange_encoded':1, 'AgeRange_encoded':4,'Localitate_encoded':1,'Tip_incalzire_principal_encoded':1,'Company_name_encoded':1,'NrMembriDeFamilie':1}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Predict the response for the selected row\n",
    "prediction = logreg.predict(row_to_predict)\n",
    "\n",
    "# Get the coefficients and intercept\n",
    "coefficients = logreg.coef_[0]\n",
    "intercept = logreg.intercept_[0]\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Coefficients for each feature:\")\n",
    "for feature, coefficient in zip(features.columns, coefficients):\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Prediction:\", prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4249/4249 [==============================] - 3s 736us/step - loss: -1157.4366 - accuracy: 0.8252 - val_loss: -4514.3926 - val_accuracy: 0.8278\n",
      "Epoch 2/10\n",
      "4249/4249 [==============================] - 3s 711us/step - loss: -13306.9209 - accuracy: 0.8217 - val_loss: -26611.4141 - val_accuracy: 0.8238\n",
      "Epoch 3/10\n",
      "4249/4249 [==============================] - 3s 711us/step - loss: -46525.1758 - accuracy: 0.8245 - val_loss: -74683.2812 - val_accuracy: 0.8260\n",
      "Epoch 4/10\n",
      "4249/4249 [==============================] - 3s 718us/step - loss: -108741.1719 - accuracy: 0.8238 - val_loss: -156306.0156 - val_accuracy: 0.8254\n",
      "Epoch 5/10\n",
      "4249/4249 [==============================] - 3s 705us/step - loss: -207400.7812 - accuracy: 0.8239 - val_loss: -279642.2812 - val_accuracy: 0.8176\n",
      "Epoch 6/10\n",
      "4249/4249 [==============================] - 3s 707us/step - loss: -348246.1562 - accuracy: 0.8241 - val_loss: -450378.1562 - val_accuracy: 0.8269\n",
      "Epoch 7/10\n",
      "4249/4249 [==============================] - 3s 712us/step - loss: -542269.5000 - accuracy: 0.8250 - val_loss: -679742.7500 - val_accuracy: 0.8252\n",
      "Epoch 8/10\n",
      "4249/4249 [==============================] - 3s 704us/step - loss: -794094.0625 - accuracy: 0.8241 - val_loss: -974379.6250 - val_accuracy: 0.8245\n",
      "Epoch 9/10\n",
      "4249/4249 [==============================] - 3s 707us/step - loss: -1112752.5000 - accuracy: 0.8234 - val_loss: -1337469.7500 - val_accuracy: 0.8248\n",
      "Epoch 10/10\n",
      "4249/4249 [==============================] - 3s 712us/step - loss: -1502906.3750 - accuracy: 0.8235 - val_loss: -1781854.7500 - val_accuracy: 0.8205\n",
      "2277/2277 [==============================] - 1s 321us/step\n",
      "Accuracy: 0.821102491933823\n",
      "Precision: 0.8844261752368062\n",
      "Recall: 0.821102491933823\n",
      "F1 Score: 0.8515887775995836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afanasichihaioglo/Desktop/ProtonEnergySolutions/Software/Hackathon/UNDP/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/afanasichihaioglo/Desktop/ProtonEnergySolutions/Software/Hackathon/UNDP/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming df_copy is your DataFrame\n",
    "features = df_copy[[\"Sex\", \"SalaryRange_encoded\", \"AgeRange_encoded\", \"Localitate_encoded\", \"Tip_incalzire_principal_encoded\", \"Company_name_encoded\", \"NrMembriDeFamilie\"]]\n",
    "target = df_copy[\"IsOutlier\"]\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=109)  # 70% training and 30% test\n",
    "\n",
    "# Standardize the features\n",
    "# nn_scaler = StandardScaler()\n",
    "# X_train_scaled = nn_scaler.fit_transform(X_train)\n",
    "# X_test_scaled = nn_scaler.transform(X_test)\n",
    "nn_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = nn_scaler.fit_transform(X_train)\n",
    "X_test_scaled = nn_scaler.transform(X_test)\n",
    "\n",
    "# Create a neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activations of neurons in the first layer for a specific sample (change the index as needed)\n",
    "sample_index = 10\n",
    "sample = X_test_scaled[sample_index].reshape(1, -1)\n",
    "activations = model.layers[0](sample).numpy()\n",
    "\n",
    "# Print the activations of neurons in the first layer\n",
    "print(\"Activations of neurons in the first layer:\")\n",
    "print(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -0.66666667  0.6        -0.94117647 -0.33333333 -0.71428571\n",
      "  -0.83333333]]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicted Probability: [[1.]]\n",
      "Predicted Class: [[1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afanasichihaioglo/Desktop/ProtonEnergySolutions/Software/Hackathon/UNDP/.venv/lib/python3.9/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert the data point to a numpy array\n",
    "import numpy as np\n",
    "new_data_point = np.array([[1, 1, 4, 1, 1, 1, 1]])\n",
    "\n",
    "# Standardize the features (assuming you used StandardScaler during training)\n",
    "new_data_point_scaled = nn_scaler.transform(new_data_point)\n",
    "print(new_data_point_scaled)\n",
    "\n",
    "# Make a prediction\n",
    "prediction_proba = model.predict(new_data_point_scaled)\n",
    "prediction_class = (prediction_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"Predicted Probability:\", prediction_proba)\n",
    "print(\"Predicted Class:\", prediction_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
